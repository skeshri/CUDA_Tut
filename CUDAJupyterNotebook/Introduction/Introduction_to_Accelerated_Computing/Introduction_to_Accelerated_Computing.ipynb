{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Accelerated Computing\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "This module serves as an introduction to NVIDIA's accelerated computing platform. You will learn:\n",
    "\n",
    "- The concept of accelerated computing\n",
    "- How GPUs are different from CPUs\n",
    "- Several different programming models for targeting NVIDIA GPUs\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "No GPU knowledge is assumed for this module. Basic knowledge of Fortran or C/C++ is assumed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heterogeneous and Accelerated Computing\n",
    "\n",
    "Heterogeneous computing involves using multiple different types of processors to accomplish a task. In this module we'll be focusing on how to use both CPUs and GPUs together to solve a problem.\n",
    "\n",
    "![](images/cpu_and_gpu.png)\n",
    "\n",
    "In common accelerated computing parlance, we refer to the CPU as the **host** and the GPU as the **device**. GPUs have their own memory (RAM) which is distinct from the CPU's memory on most accelerated computing architectures. We refer to the CPU's memory as **host memory** and the GPU's memory as **device memory**.\n",
    "\n",
    "![](images/accelerated_computing.png)\n",
    "\n",
    "In accelerated computing we take the compute intensive parts of the application code, usually the part of the code where we are spending the most time, and parallelize that for execution on a GPU. The remainder of the code (which may be the vast majority of the number of lines of code) remain on the CPU. Ideally, the part of the code that remains on the CPU is traditional serial code such as input/output operations and high level control flow. The part of the code that makes the most sense to put on the GPU is the operations involving a substantial amount of compute work (typically integer or floating point mathematical operations). In particular, the work that goes onto the GPU must be highly parallelizable. This division of labor reflects the fact that CPUs have complex compute cores (that can execute many types of instructions) but relatively few of them, while GPUs have simple compute cores (that execute a more limited instruction set) but relatively many of them. A modern high end server class CPU typically has on the order of tens of cores, while a modern high end server class GPU has thousands of cores, so we really must adopt some method of parallel computing to effectively use the GPU.\n",
    "\n",
    "![](images/gpu-devotes-more-transistors-to-data-processing.png)\n",
    "\n",
    "The image above demonstrates the fundamental difference between a CPU and GPU: given the same amount of physical area on a computer chip (or, thought of another way, the number of transistors on the chip), GPUs preferentially devote their area to large amounts of fairly simplistic data processing components, while CPUs tend to devote more of their area to large caches and complex control flow components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NVIDIA HPC SDK\n",
    "\n",
    "<center><img src=\"images/hpc-sdk.png\"/>\n",
    "\n",
    "The NVIDIA [HPC SDK](https://developer.nvidia.com/hpc-sdk) is a comprehensive suite of developer tools for accelerated HPC. It includes the C, C++, and Fortran compilers, libraries, and analysis tools necessary for developing HPC applications on the NVIDIA platform.\n",
    "\n",
    "<center><img src=\"images/hpc-sdk-2.png\"/>\n",
    "\n",
    "Programming with the HPC SDK can involve one of several approaches, depending on your concern for productivity and performance. Options range from native acceleration in the languages -- [C++ parallel algorithms](https://docs.nvidia.com/hpc-sdk/compilers/c++-parallel-algorithms/index.html), starting with C++17, and Fortran `do concurrent` starting with Fortran 2008 -- to directive-based programming (hints provided to the compiler to accelerate a specific compute workload) and low-level programming in CUDA to achieve maximum performance.\n",
    "    \n",
    "<center><img src=\"images/hpc-sdk-3.png\"/>\n",
    "\n",
    "For many problems, off-the-shelf vendor libraries are an easy way to achieve acceleration, particularly with linear algebra problems, FFTs, and random number generation. NVIDIA provides several math libraries that offer drop-in acceleration on GPUs, often with similar or identical APIs to what you are familiar with on CPUs.\n",
    "\n",
    "<center><img src=\"images/hpc-sdk-4.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "In this module we have learned:\n",
    "\n",
    "- The difference between host (CPU) and device (GPU) architectures\n",
    "\n",
    " \n",
    "- Accelerated computing models offered by the NVIDIA HPC SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Study\n",
    "\n",
    "[NVIDIA HPC Developer Site](https://developer.nvidia.com/hpc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Materials\n",
    "\n",
    "You can download this notebook using the `File > Download as > Notebook (.ipnyb)` menu item. Source code files can be downloaded from the `File > Download` menu item after opening them."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
