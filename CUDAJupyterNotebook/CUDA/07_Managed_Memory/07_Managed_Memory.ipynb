{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managed Memory\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "In this lab we will learn about managed memory, a solution to dealing with data management that substantially improves productivity. Particularly, we will:\n",
    "\n",
    "- Learn what Unified Memory (managed memory) is and how to use it\n",
    "- Understand the performance limitations and how to avoid them if desired\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "It is assumed that participants understand data management using `cudaMalloc()`, `cudaFree()`, and `cudaMemcpy`. (Although managed memory provides an alternative to these methods, we contrast it with the explicit memory management APIs.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our first introduction to CUDA, we demonstrated a three-step processing workflow for CUDA. \n",
    "\n",
    "![](images/simple_processing_flow.png)\n",
    "\n",
    "Wouldn't it be nice if we didn't explicitly have to do steps 1 and 3?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unified Memory\n",
    "\n",
    "Starting with CUDA 6 and the Kepler generation of GPUs, NVIDIA introduced Unified Memory (synonymous with \"managed memory\"). When using unified memory, you can allocate pointers that can be deferenced in *both* host and device code.\n",
    "\n",
    "![](images/unified_memory_cuda_6_kepler.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then in CUDA 8 with the Pascal generation of GPUs, Unified Memory became greatly expanded, to the model we have today.\n",
    "\n",
    "![](images/unified_memory_cuda_8_pascal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, we can take code that looks this with manual data management:\n",
    "\n",
    "![](images/simplified_memory_management_before.png)\n",
    "\n",
    "and turn it into:\n",
    "\n",
    "![](images/simplified_memory_management_after.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unified Memory Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a code sample to understand how the unified memory implementation works. First, we'll discuss how it works on Linux, for Pascal generation and later GPUs.\n",
    "\n",
    "In this code sample, we allocate data with the unified memory allocator, `cudaMallocManaged()`, and the allocated pointer is then accessed on the CPU (using `memset()`), then on the CPU (in the function `useData()`).\n",
    "\n",
    "```\n",
    "__global__\n",
    "void setValue(int *ptr, int index, int val) \n",
    "{\n",
    "  ptr[index] = val;\n",
    "}\n",
    "\n",
    "\n",
    "void foo(int size) {\n",
    "  char *data;\n",
    "  cudaMallocManaged(&data, size);\n",
    "\n",
    "  memset(data, 0, size);\n",
    "\n",
    "  setValue<<<...>>>(data, size/2, 5);\n",
    "  cudaDeviceSynchronize();\n",
    "\n",
    "  useData(data);\n",
    "  \n",
    "  cudaFree(data);\n",
    "}\n",
    "```\n",
    "\n",
    "The animation below describes what happens. On both CPUs and GPUs, on the vast majority of operating systems, virtual memory is allocated in chunks called *pages*, and when an element of its data is touched, the processor asks if there is a corresponding physical memory location that the virtual memory is backed by. If there is no such location known to the processor's memory management unit (MMU), we have a *page fault*, and the system must provide physical memory to match the virtual memory. This is true on both CPUs and GPUs when using unified memory; in the code above, the CPU will page fault when `memset()` is called, and some pages will be filled. Then, when `setValue()` is executed on the GPU, the GPU will page fault, and the CUDA driver will *migrate* the pages from the CPU to the GPU. After the kernel completes, the `useData()` function on the host will page fault because the pages no longer reside in CPU RAM, and the CUDA driver will migrate the pages back to the CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/managed_memory_page_faulting.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to allocate more managed memory than there is GPU memory:\n",
    "\n",
    "```\n",
    "void foo() {\n",
    "  // Assume GPU has 16 GB memory\n",
    "  // Allocate 64 GB\n",
    "  char *data;\n",
    "  // be careful with size type:\n",
    "  size_t size = 64ULL * 1024 * 1024 * 1024;\n",
    "  cudaMallocManaged(&data, size);\n",
    "}\n",
    "```\n",
    "\n",
    "Only a subset of the pages will reside on the GPU at any one time. If the GPU memory is full and new pages are requested to move over, the driver will *evict* some pages back to the CPU. This enables you to work on datasets that are larger than can fit in the GPU -- however, achieving reasonable performance on this case may require some effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Let's test this oversubscription idea to verify that it works. [exercises/vector_addition.cu](exercises/vector_addition.cu) is a vector addition implementation that currently uses arrays which are much smaller than GPU memory. Change the length of the arrays so that the total amount of dynamic memory allocated is larger than GPU memory. Does the time it takes to run the code proportionately or disproportionately increase as you begin to overflow GPU memory?\n",
    "\n",
    "For each of the sizes you try, calculate the total size of the data you've allocated and estimate an effective bandwidth of the kernel. How does it compare to the DRAM bandwidth of the GPU? Don't worry if it is slow -- we will talk about Unified Memory performance later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=native -o vector_addition exercises/vector_addition.cu\n",
    "%time !./vector_addition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, it is also possible to *concurrently* access a unified memory allocation on both CPU and GPU.\n",
    "\n",
    "```\n",
    "__global__ void mykernel(char *data) {\n",
    "  data[1] = ‘g’;\n",
    "}\n",
    "\n",
    "void foo() {\n",
    "  char *data;\n",
    "  cudaMallocManaged(&data, 2);\n",
    "\n",
    "  mykernel<<<...>>>(data);\n",
    "  // no synchronize here\n",
    "  data[0] = ‘c’;\n",
    "\n",
    "  cudaFree(data);\n",
    "}\n",
    "```\n",
    "\n",
    "However, the fact that this is possible does not exempt you from considering race conditions. Generally, the unified memory implementation does not enforce ordering or visibility guarantees for concurrent CPU-GPU accesses. In the example above, without a synchronization after the kernel, there is no guarantee about what the value of either `data[0]` or `data[1]` will be. (Though if they were not on the same memory page, this example may work as desired.)\n",
    "\n",
    "With that said, on Pascal and later GPUs, system-wide atomic operations are possible. These can be combined with CPU atomic operations for joint CPU-GPU atomic operations.\n",
    "\n",
    "```\n",
    "__global__ void mykernel(int *addr) {\n",
    "  // GPU atomic:\n",
    "  atomicAdd_system(addr, 10);\n",
    "}\n",
    "\n",
    "void foo() {\n",
    "  int *addr;\n",
    "  cudaMallocManaged(addr, 4);\n",
    "  *addr = 0;\n",
    "\n",
    "  mykernel<<<...>>>(addr);\n",
    "  // CPU atomic:\n",
    "  __sync_fetch_and_add(addr, 10); \n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few contexts where this unified memory implementation doesn't exist, and instead there is a less powerful implementation. In particular, that's pre-Pascal generation GPUs, all Jetson GPUs, and on Windows. In those situations:\n",
    "\n",
    "- When you launch a kernel, *all* managed data migrates immediately to the GPU\n",
    "- CPU page faulting works as normal\n",
    "- No concurrent access to unified memory between CPUs and GPUs is permitted\n",
    "- THe limit of allocatable unified memory is the size of the GPU DRAM\n",
    "\n",
    "![](images/pre_pascal_unified_memory_page_faulting.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unified Memory Use Cases\n",
    "\n",
    "Unified Memory is primarily designed around productivity. It is nice that you don't have to know where and when data motion occurs -- the CUDA driver will figure this out for you. However, there are some particular use cases where unified memory truly shines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we define a struct as follows:\n",
    "\n",
    "```\n",
    "struct dataElem {\n",
    "  int key;\n",
    "  int len;\n",
    "  char *name;\n",
    "}\n",
    "```\n",
    "\n",
    "If we have an instance of this struct on the host, and we want to copy it to the device, we need to allocate a device copy of the data and also all the data it points to:\n",
    "\n",
    "```\n",
    "void launch(dataElem *elem) {\n",
    "  dataElem *d_elem;\n",
    "  char *d_name;\n",
    "\n",
    "  int namelen = strlen(elem->name) + 1;\n",
    "\n",
    "  // Allocate storage for struct and name\n",
    "  cudaMalloc(&d_elem, sizeof(dataElem));\n",
    "  cudaMalloc(&d_name, namelen);\n",
    "\n",
    "  // Copy up each piece separately, including new “name” pointer value\n",
    "  cudaMemcpy(d_elem, elem, sizeof(dataElem), cudaMemcpyHostToDevice);\n",
    "  cudaMemcpy(d_name, elem->name, namelen, cudaMemcpyHostToDevice);\n",
    "  cudaMemcpy(&(d_elem->name), &d_name, sizeof(char*), cudaMemcpyHostToDevice);\n",
    "\n",
    "  // Finally we can launch our kernel, but CPU and GPU use different copies of “elem”\n",
    "  kernel<<< ... >>>(d_elem);\n",
    "}\n",
    "```\n",
    "\n",
    "Obviously, this can be very tedious if there are many data elements to copy. However, if all of the data has been allocated with managed memory, we can simply launch the kernel:\n",
    "\n",
    "```\n",
    "void launch(dataElem *elem) {\n",
    "  kernel<<< ... >>>(elem);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linked List\n",
    "\n",
    "Another example would be a linked list shared between the CPU and GPU. Since a linked list is a chain of pointers, logic that dealt with consistency between the CPU and GPU accurately would be fairly complex. With managed memory, we don't have to worry about that, we can just use the data when it is needed.\n",
    "\n",
    "![](images/linked_list.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Let's experiment with this linked list idea. In [exercises/linked_list.cu](exercises/linked_list.cu) you will find an example code that attempts to print out a specific member of a linked list on both the CPU and GPU. However, because the data is not accessible on the GPU, the kernel will fail. Rewrite this code to use managed memory (this should just be a one-line change) and verify that you get the expected result. The solution can be found in [solutions/linked_list.cu](solutions/linked_list.cu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=native -o linked_list exercises/linked_list.cu\n",
    "!./linked_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Memory Management for C++ Objects\n",
    "\n",
    "Managed memory also helps substantially in managing complex C++ classes. We can declare a base class `Managed`:\n",
    "\n",
    "```\n",
    "class Managed {\n",
    "public:\n",
    "  void *operator new(size_t len) {\n",
    "    void *ptr;\n",
    "    cudaMallocManaged(&ptr, len);\n",
    "    cudaDeviceSynchronize();\n",
    "    return ptr;\n",
    "  }\n",
    "\n",
    "  void operator delete(void *ptr) {\n",
    "    cudaDeviceSynchronize();\n",
    "    cudaFree(ptr);\n",
    "  }\n",
    "};\n",
    "```\n",
    "\n",
    "and then other classes can derive from it:\n",
    "\n",
    "```\n",
    "// Deriving from “Managed” allows pass-by-reference to kernel\n",
    "class String : public Managed {\n",
    "  int length;\n",
    "  char *data;\n",
    "\n",
    "public:\n",
    "  // Unified memory copy constructor allows pass-by-value to kernel\n",
    "  String (const String &s) {\n",
    "    length = s.length;\n",
    "    cudaMallocManaged(&data, length);\n",
    "    memcpy(data, s.data, length);\n",
    "  }\n",
    "\n",
    "  // ...\n",
    "};\n",
    "```\n",
    "\n",
    "Here we also implement a copy constructor that allocates the data with managed memory.\n",
    "\n",
    "This can also be used for structs:\n",
    "\n",
    "```\n",
    "class dataElem : public Managed {\n",
    "public:\n",
    "  int prop1;\n",
    "  int prop2;\n",
    "  String name;\n",
    "};\n",
    "\n",
    "...\n",
    "\n",
    "dataElem *data = new dataElem[N];\n",
    "\n",
    "...\n",
    "\n",
    "// C++ now handles our deep copies\n",
    "kernel<<< ... >>>(data);\n",
    "\n",
    "```\n",
    "\n",
    "Now we can have kernels that both pass by reference and pass by value:\n",
    "\n",
    "```\n",
    "// Pass-by-reference version\n",
    "__global__ void kernel_by_ref(dataElem &data) { ... }\n",
    "\n",
    "// Pass-by-value version\n",
    "__global__ void kernel_by_val(dataElem data) { ... }\n",
    "\n",
    "int main(void) {\n",
    "  dataElem *data = new dataElem;\n",
    "  ...\n",
    "  // pass data to kernel by reference\n",
    "  kernel_by_ref<<<1,1>>>(*data);\n",
    "\n",
    "  // pass data to kernel by value -- this will create a copy\n",
    "  kernel_by_val<<<1,1>>>(*data);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Considerations\n",
    "\n",
    "Consider the following scenario:\n",
    "\n",
    "```\n",
    "__global__ void kernel(float *data) {\n",
    "  int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "  data[idx] = val;\n",
    "}\n",
    "\n",
    "...\n",
    "\n",
    "int n = 256 * 256;\n",
    "float *data;\n",
    "cudaMallocManaged(&data, n * sizeof(float);\n",
    "kernel<<<256, 256>>>(data);\n",
    "```\n",
    "\n",
    "This kernel runs *much* slower than the case where we explicitly allocate and copy the data with `cudaMalloc()` and `cudaMemcpy()`. The reason is that every thread will trigger a page fault, which has some service overhead, and this will result in many inefficient, small copies rather than an efficient, bulk copy.\n",
    "\n",
    "If this overhead significantly affects your application performance, you can trigger a bulk copy with:\n",
    "\n",
    "```\n",
    "cudaMemPrefetchAsync(ptr, length, destDevice);\n",
    "```\n",
    "\n",
    "(As is suggested by the name, the resulting copy happens asynchronously, like kernel execution.)\n",
    "\n",
    "In the code sample above, that would look like:\n",
    "\n",
    "```\n",
    "// Note that the default device is 0\n",
    "cudaMemPrefetchAsync(data, ds, 0); \n",
    "kernel<<<256, 256>>>(data);\n",
    "cudaMemPrefetchAsync(data, ds, cudaCpuDeviceId); // copy back to host\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Consider the code in [exercises/array_increment.cu](exercises/array_increment.cu), which allocates an array on the host and then increments every value in the array on the device.\n",
    "\n",
    "Let's compile and run the code as-is, noting the duration of the kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=native -o array_increment exercises/array_increment.cu\n",
    "!nsys profile --stats=true ./array_increment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now convert the code to use unified memory. First, replace `cudaMalloc()` with `cudaMallocManaged()`, and eliminate the calls to `cudaMemcpy()`. Again, note the kernel runtime. Then, use `cudaMemPrefetchAsync()` to improve the performance, and validate that with your profiling results. Look at [solutions/array_increment.cu](solutions/array_increment.cu) if you need help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=native -o array_increment exercises/array_increment.cu\n",
    "!nsys profile --stats=true ./array_increment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you like, you can also experiment with running the kernel many times in a row (say, 10000). This is representative of many real world use cases where the data is transferred to the device once and stays there for a long time. What can we say about the fraction of the time spent in kernels versus memory operations in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unified Memory Hints\n",
    "\n",
    "You can advise the unified memory runtime on expected memory access behaviors with:\n",
    "\n",
    "```\n",
    " cudaMemAdvise(ptr, count, hint, device);\n",
    "```\n",
    "\n",
    "Some available \"hints\" are:\n",
    "\n",
    "- `cudaMemAdviseSetReadMostly`: specifies read duplication (both CPU and GPU have a copy)\n",
    "- `cudaMemAdviseSetPreferredLocation`: suggest best location (data will stay here if possible)\n",
    "- `cudaMemAdviseSetAccessedBy`: suggest a page mapping (to avoid page faults on later access)\n",
    "\n",
    "Note that these hints don’t trigger data movement by themselves. For more details, see the [CUDA Runtime API documentation](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1ge37112fc1ac88d0f6bab7a945e48760a)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "In this lab we learned:\n",
    "\n",
    "- How to allocate Unified Memory (managed memory) with `cudaMallocManaged()`\n",
    "- The on-demand paging nature of Unified Memory transfers on Linux and Pascal+\n",
    "- How to asynchronously copy it to the device (or back to the host) with `cudaMemPrefetchAsync()`\n",
    "- Some ideas for how to optimize applications that use Unified Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Study\n",
    "\n",
    "[NVIDIA Developer Blog: Unified Memory for CUDA Beginners](https://devblogs.nvidia.com/unified-memory-cuda-beginners/)\n",
    "\n",
    "[NVIDIA Developer Blog: Unified Memory in CUDA 6](https://devblogs.nvidia.com/unified-memory-in-cuda-6/)\n",
    "\n",
    "[NVIDIA Developer Blog: Maximizing Unified Memory Performance in CUDA](https://devblogs.nvidia.com/maximizing-unified-memory-performance-cuda/)\n",
    "\n",
    "[GTC 2018: Everything You Need to Know About Unified Memory](http://on-demand.gputechconf.com/gtc/2018/presentation/s8430-everything-you-need-to-know-about-unified-memory.pdf)\n",
    "\n",
    "[CUDA Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-unified-memory-programming-hd)\n",
    "\n",
    "CUDA Samples:  conjugateGradientUM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Materials\n",
    "\n",
    "You can download this notebook using the `File > Download as > Notebook (.ipnyb)` menu item. Source code files can be downloaded from the `File > Download` menu item after opening them."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
