{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concurrency with Streams\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "In this lab we will learn about how to achieve concurrency in CUDA using CUDA streams. (Concurrency can also be achieved by using multiple GPUs; we will explore this in a later lab.) Topics include:\n",
    "\n",
    "- CUDA streams: what they are, and how to use them\n",
    "- Achieving asynchronous H2D/D2H copies and kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the three-step processing workflow for CUDA:\n",
    "\n",
    "![](images/simple_processing_flow.png)\n",
    "\n",
    "From a timeline perspective, this looks like:\n",
    "\n",
    "![](images/serial_processing_flow.png)\n",
    "\n",
    "That means that a substantial portion of the timeline is not spent on compute, and we're not utilizing the GPU efficiently. We would like instead to be able to do:\n",
    "\n",
    "![](images/concurrent_processing_flow.png)\n",
    "\n",
    "Then a much higher percentage of the time spent would involve computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pinned (Non-Pageable) Memory\n",
    "\n",
    "CUDA enables the allocation of host *pinned memory*. Pinned memory enables:\n",
    "\n",
    "- Faster host <-> device copies\n",
    "- Memcopies from CPU to GPU that are asynchronous\n",
    "- Memcopies from GPU to CPU that are asynchronous\n",
    "- Direct access within a CUDA kernel\n",
    "    - For this reason, pinned memory is also called \"zero-copy\" memory in some sources\n",
    "\n",
    "It is used by calling the API:\n",
    "\n",
    "```\n",
    "cudaMallocHost(&ptr, length);\n",
    "cudaFreeHost(ptr);\n",
    "```\n",
    "\n",
    "CUDA pinned memory is *page-locked* on the host. That means the virtual to physical address translation is fixed; the memory is not subject to normal OS paging. (Since virtual memory paging is often important to CPU memory performance, and pinning memory subtracts memory from the pool of pageable memory, one should be cautious in how much pinned memory is allocated.) This allows the GPU to directly dereference the pointer in a kernel (since the GPU knows directly where to access it, and can do so while bypassing the CPU).\n",
    "\n",
    "If pageable memory has already been allocated on the CPU, it can be pinned through CUDA APIs[<sup>1</sup>](#footnote1):\n",
    "\n",
    "```\n",
    "cudaHostRegister(ptr, length, cudaHostRegisterDefault);\n",
    "cudaHostUnregister(ptr);\n",
    "```\n",
    "\n",
    "Notably, the asynchronous memcopy operation that we are about to discuss, `cudaMemcpyAsync()`, is only fully asynchronous when writing to or reading from a pinned memory buffer. If the destination or source buffer is normal host pageable memory, the CUDA driver cannot complete the operation in a fully asynchronous manner, since it depends on the CPU handling the pageable memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Let's play around with pinned memory. Our claim is that for both synchronous and asynchronous copies, it should be faster to copy pinned memory to the device. (There is a tradeoff of course, in the sense that it takes longer to allocate pinned memory than to allocate pageable memory.) Let's verify that this is true. [exercises/pinned.cu](exercises/pinned.cu) currently makes a host to device and then device to host copy of some data, where the host data uses pageable memory. Profile the code to understand how long each of these operations took. Then convert the pageable allocations to pinned allocations (check [solutions/pinned.cu](solutions/pinned.cu) for answers) and re-collect the profile to see whether the memcopy operations got faster. If so, by how much? Does the speedup compensate for the additional time we spent pinning the memory? If not, how many memcopies would we have to do to amortize that cost out? Do these conclusions depend on the value of `N`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=native -o pinned exercises/pinned.cu\n",
    "!nsys profile --stats=true pinned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA Streams\n",
    "\n",
    "When we launch a kernel with\n",
    "\n",
    "```\n",
    "kernel<<<blocks, threads>>>();\n",
    "```\n",
    "\n",
    "we have already learned that the kernel launches asynchronously with respect to the host, and we need to use (say) `cudaDeviceSynchronize()` to wait until the kernel is completed. Certain other CUDA APIs such as `cudaMemcpy()` are implicitly synchronous with respect to these kernel launches.\n",
    "\n",
    "CUDA has the concept of *streams*, which are in-order work queues. Items of work (usually kernels or memcopies) submitted to a stream are executed in order, and if two items are submitted to the stream asynchronously, the second one cannot begin execution until the first one completes. Streams also obey the additional rule that two items of work submitted to *different* streams have no ordering prescribed by CUDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Stream\n",
    "\n",
    "CUDA provides a *default stream* to which asynchronous work is issued in the case of asynchronous APIs which do not require a stream. For example, if we do\n",
    "\n",
    "```\n",
    "kernel1<<<blocks, threads>>>();\n",
    "kernel2<<<blocks, threads>>>();\n",
    "```\n",
    "\n",
    "both kernels execute in the default stream, and `kernel2` does not start until `kernel1` completes. \n",
    "\n",
    "We can intersperse asynchronous memcopies in this sequence of operations, for example:\n",
    "\n",
    "```\n",
    "cudaMemcpyAsync(d_ptr1, h_ptr1, length, cudaMemcpyHostToDevice);\n",
    "kernel1<<<blocks, threads>>>(d_ptr1);\n",
    "cudaMemcpyAsync(h_ptr1, d_ptr1, length, cudaMemcpyDeviceToHost);\n",
    "\n",
    "cudaMemcpyAsync(d_ptr2, h_ptr2, length, cudaMemcpyHostToDevice);\n",
    "kernel2<<<blocks, threads>>>(d_ptr2);\n",
    "cudaMemcpyAsync(h_ptr2, d_ptr2, length, cudaMemcpyDeviceToHost);\n",
    "\n",
    "cudaDeviceSynchronize();\n",
    "```\n",
    "\n",
    "and these six operations will occur in sequence on the GPU. The default stream has the special rule that it is synchronous with respect to all CUDA work in any stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA APIs and Streams\n",
    "\n",
    "Many CUDA APIs (including some that we have already seen) explicitly accept a CUDA stream as an optional argument. For example, the extended version of the triple-chevron launch syntax is\n",
    "\n",
    "```\n",
    "kernel<<<blocks, threads, smem, stream>>>();\n",
    "```\n",
    "\n",
    "where `smem` is the amount of dynamically allocated shared memory to use, and `stream` is the handle for the CUDA stream you want to use.\n",
    "\n",
    "Similarly, memcopy operations have this as well:\n",
    "\n",
    "```\n",
    "cudaMemcpyAsync(dest, src, length, direction, stream);\n",
    "cudaMemPrefetchAsync(ptr, length, device, stream);\n",
    "```\n",
    "\n",
    "The default stream is denoted with the special argument of `0`. That is,\n",
    "\n",
    "```\n",
    "kernel<<<blocks, threads, 0, 0>>>();\n",
    "```\n",
    "\n",
    "submits the kernel to the default stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Default Streams\n",
    "\n",
    "CUDA permits you to create streams of your own for asynchronous execution.\n",
    "\n",
    "```\n",
    "cudaStream_t stream1, stream2;\n",
    "cudaStreamCreate(&stream1);\n",
    "cudaStreamCreate(&stream2);\n",
    "\n",
    "...\n",
    "\n",
    "cudaStreamDestroy(stream1);\n",
    "cudaStreamDestroy(stream2);\n",
    "```\n",
    "\n",
    "After the streams have been created, they can be used in the CUDA APIs:\n",
    "\n",
    "```\n",
    "cudaMemcpyAsync(d_ptr1, h_ptr1, length, cudaMemcpyHostToDevice, stream1);\n",
    "kernel<<<blocks, threads, 0, stream2)(d_ptr2);\n",
    "```\n",
    "\n",
    "In this case, the asynchronous memcopy may be executed before, after, or concurrently with the kernel.\n",
    "\n",
    "If you need to synchronize with respect to the work in a given non-default stream, you can use:\n",
    "\n",
    "```\n",
    "cudaStreamSynchronize(stream);\n",
    "```\n",
    "\n",
    "See below for some examples of possible execution scenarios. In this figure, `K` denotes kernel while `M` denotes memcopy, and the subsequent integer indicates which of two streams that operation has been submitted to (the order in the text corresponds to the order the operations are launched in).\n",
    "\n",
    "![](images/stream_examples.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "[exercises/user_stream.cu](exercises/user_stream.cu) is the same code we just ended with above. It should correctly set an array to the value `1`. Refactor this code by launching all the work in a non-default stream, so that the `cudaMemcpy` calls become `cudaMemcpyAsync`, and the kernel launch passes the stream as its fourth argument. Use `cudaStreamSynchronize()` instead of `cudaDeviceSynchronize()` to ensure all work is completed. Verify that the code still works correctly. Look at [solutions/user_stream.cu](solutions/user_stream.cu) if you need a hint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=native -o user_stream exercises/user_stream.cu\n",
    "!./user_stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Processing Example\n",
    "\n",
    "Suppose we're processing on vector data, for example adding one vector to another or doing an axpy operation. When using the default stream, we get:\n",
    "\n",
    "```\n",
    "cudaMemcpy(d_x, h_x, size_x, cudaMemcpyHostToDevice);\n",
    "kernel<<<blocks, threads>>>(d_x, d_y, N);\n",
    "cudaMemcpy(h_y, d_y, size_y, cudaMemcpyDeviceToHost);\n",
    "```\n",
    "\n",
    "![](images/vector_processing_serial.png)\n",
    "\n",
    "Now imagine that we have an array `cudaStream_t streams[c]` with `c` streams. Suppose that the length of the arrays are `size_x` and `size_y` respectively. We can decompose the problem into `c` chunks, and then achieve overlap between memcopy and compute, with the kernel for a given stream executing simultaneously with the memcopy on the next stream.\n",
    "\n",
    "```\n",
    "for (int i = 0, i < c; i++) {\n",
    "    size_t offx = (size_x / c) * i;\n",
    "    size_t offy = (size_y / c) * i;\n",
    "    cudaMemcpyAsync(d_x + offx, h_x + offx, size_x / c, cudaMemcpyHostToDevice, streams[i % ns]);\n",
    "    kernel<<<blocks / c, threads, 0, streams[i % ns]>>>(d_x + offx, d_y + offy, N / c);\n",
    "    cudaMemcpyAsync(h_y + offy, d_y + offy, size_y / c, cudaMemcpyDeviceToHost, streams[i % ns]);\n",
    "}\n",
    "```\n",
    "\n",
    "Here is the two-stream example:\n",
    "\n",
    "![](images/vector_processing_concurrent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above workflow is also possible with managed memory:\n",
    "\n",
    "```\n",
    "for (int i = 0, i < c; i++) {\n",
    "    size_t offx = (size_x / c) * i;\n",
    "    size_t offy = (size_y / c) * i;\n",
    "    cudaMemPrefetchAsync(x + offx, size_x / c, 0, streams[i % ns]);\n",
    "    kernel<<<blocks / c, threads, 0, streams[i % ns]>>>(x + offx, y + offy, N / c);\n",
    "    cudaMemPrefetchAsync(y + offy, size_y / c, cudaCpuDeviceId, streams[i % ns]);\n",
    "}\n",
    "```\n",
    "\n",
    "Stream semantics guarantee that the prefetching of the data completes before the kernel begins execution.\n",
    "\n",
    "A caveat with managed memory is that the API call itself is often much higher latency than `cudaMemcpyAsync()` (since [the operation requires updating CPU and GPU page tables](https://developer.nvidia.com/blog/maximizing-unified-memory-performance-cuda/)). Depending on the length of the copies and the kernels, this can sometimes appear as a latency bubble that disrupts the achievement of fully asynchronous work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactions between Non-Default Streams and the Default Stream\n",
    "\n",
    "We said above that two operations in different streams are asynchronous with respect to each other. An exception is the default stream, which is fully synchronous with respect to operations in other streams:\n",
    "\n",
    "![](images/default_stream.png)\n",
    "\n",
    "This means that operations launched on the default stream wait until all previously launched kernels (on any stream) are complete, and any kernels launched on any stream launched after the default stream kernel do not start until that kernel is complete. (Though it is possible to create non-default streams in such a way that they do not block with respect to the default stream; see [cudaStreamCreateWithFlags()](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.html#group__CUDART__STREAM_1gb1e32aff9f59119e4d0a9858991c4ad3). It is also possible to make the default stream a regular stream that doesn't have this special synchronization behavior using the nvcc flag [--default-stream per-thread](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#default-stream)).\n",
    "\n",
    "For this reason, the default stream should generally be avoided when you are trying to closely manage scenarios for concurrency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "The code in [exercises/overlap.cu](exercises/overlap.cu) performs a simple element-wise calculation on a vector. Compile, run, and profile the code as a first step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=native -o overlap exercises/overlap.cu\n",
    "!nsys profile -f true -o overlap --stats=true overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you open the report (overlap.qdrep) in the Nsight Systems UI, you should see a sequential set of operations: copy the data from host to device, run the kernel, then copy the data from device to host.\n",
    "\n",
    "The code also has a section to do the same operation with streams, but it is currently ifdef'ed out (because it is not completed. Complete that code section, dealing with the `FIXME` locations, then compile with that section enabled (`-DUSE_STREAMS`) and run again. (Check [the solution](solutions/overlap.cu) if you need help.) Does the version with streams complete faster, as we hoped? What can we say based on the stdout profiling report?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=native -o overlap_with_streams -DUSE_STREAMS exercises/overlap.cu\n",
    "!nsys profile -f true -o overlap_with_streams --stats=true overlap_with_streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, inspect the report (overlap_with_streams.qdrep) in the Nsight Systems UI and see if you can visually observe kernels overlapping with memory copies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Host Callbacks\n",
    "\n",
    "CUDA streams are most commonly used for managing asynchronous memcopies and kernels, but it is also possible to insert asychronous host operations (callbacks) onto a stream. This is useful for performing some work that depends on the outcome of GPU kernels. These callbacks obey stream semantics: they do not begin execution until previous operations on the stream complete. A worker thread is spawned by CUDA to perform the host callback. The API is:\n",
    "\n",
    "```\n",
    "// Can pass data to the function\n",
    "cudaLauncHostFunc(stream, function, data);\n",
    "```\n",
    "\n",
    "A limitation of callbacks is that they may not themselves call into the CUDA API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "In this lab we learned:\n",
    "\n",
    "- What CUDA streams are and how to use them\n",
    "- The difference between the default stream and user-created (non-default) streams\n",
    "- How to write asynchronous workflows that can overlap compute and memcopies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Study\n",
    "\n",
    "[NVIDIA Developer Blog: Concurrency with Unified Memory](https://devblogs.nvidia.com/maximizing-unified-memory-performance-cuda/)\n",
    "\n",
    "[CUDA Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#asynchronous-concurrent-execution)\n",
    "\n",
    "CUDA Samples:  concurrentKernels, simpleStreams, asyncAPI, simpleCallbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Materials\n",
    "\n",
    "You can download this notebook using the `File > Download as > Notebook (.ipnyb)` menu item. Source code files can be downloaded from the `File > Download` menu item after opening them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Footnotes\n",
    "\n",
    "<span id=\"footnote1\">1</span>: The Linux kernel can also page-lock memory with [mlock](https://man7.org/linux/man-pages/man2/mlock.2.html). However, this cannot be used as a replacement for CUDA pinned memory because the pinned pages also need to be registered with the CUDA driver."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
